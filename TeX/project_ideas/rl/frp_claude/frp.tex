\documentclass[11pt]{article}

% --- Layout ---
\usepackage[letterpaper, margin=1.5cm]{geometry}
\usepackage{libertine}
\usepackage[T1]{fontenc}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0.65em}

\usepackage{amsmath, amssymb, amsthm, mathtools}
\let\Bbbk\relax \let\openbox\relax
\usepackage[libertine]{newtxmath}
\usepackage[colorlinks=true, urlcolor=blue, linkcolor=blue, citecolor=blue]{hyperref}
\usepackage{cleveref}
\usepackage{braket}
\usepackage{url}
\usepackage{graphicx, caption, wrapfig, booktabs}

\DeclareMathOperator{\Tr}{Tr}

\title{Persona world models: understanding and mitigating emergent misalignment through belief-state interventions}
\author{}
\date{}

\begin{document}
\maketitle

\section{Project Description}
We develop a mathematical framework for model personas---Semi-Factored Processes---and use it to build more precise persona steering tools and to investigate training-time interventions that reduce susceptibility to emergent misalignment.

\section{Abstract}
% Target: 100-200 words (~170 words)
Emergent misalignment (EM)---where fine-tuning on narrow tasks produces broadly misaligned models---is a critical safety risk, yet we lack theoretical understanding of why it occurs or how to prevent it. We introduce Semi-Factored Processes (SFPs), a class of generalized Hidden Markov Models that captures the core phenomena of EM in a fully interpretable setting. In SFPs, model personas and capabilities occupy distinct but correlated subspaces of a world model, allowing us to precisely study how narrow fine-tuning propagates misalignment.
Our key result so far: \textit{belief-state steering}---interventions defined with respect to world-model components---can differentially target personas with $>$1000:1 specificity versus spillover to unrelated behaviors, compared to conventional steering which entangles persona and task.
In the remaining program, we will demonstrate that belief-state steering can reverse misalignment fine-tuning and investigate \textit{persona engineering}---modifying persona structure to reduce EM susceptibility. During the extension, we validate these findings in 1--10B parameter language models and prepare results for publication.


\section{Background}
% Target: 250-350 words (~320 words)

\textbf{Problem and safety relevance.}
Emergent misalignment occurs when fine-tuning models on narrow misaligned tasks---such as writing insecure code---causes broadly misaligned behavior, including deception and power-seeking \cite{betley2025emergent}. This has been documented across multiple model families and scales. While persona-based interventions (persona vectors \cite{anthropic2025persona}, inoculation prompting \cite{tan2025inoculationprompting}, character training \cite{maiya2025opencharactertrainingshaping}) have shown empirical promise in reducing misalignment, they lack theoretical grounding in \textit{how} personas form, interact with capabilities, or break down under fine-tuning. This means current interventions may fail unpredictably---labs cannot systematically predict which fine-tuning procedures will trigger EM, nor design personas robust to it.

\textbf{Prior work.}
Recent work establishes key empirical facts: EM is mediated by persona features identifiable via SAEs \cite{wang2025persona}; models can be steered into and out of misaligned behavior via low-rank linear interventions \cite{soligo2025convergent, turner2025model}; and personas are increasingly central to alignment post-training \cite{lu2026assistantaxissituatingstabilizing}. However, no existing framework explains \textit{why} narrow fine-tuning produces broad misalignment, or provides principled guidance for robust persona design.

\textbf{Our approach.}
We address this gap using the recently developed theory of factored \cite{shai2026transformerslearnfactoredrepresentations} and non-ergodic \cite{riechers2025nexttokenpretrainingimpliesincontext} belief states. We introduce Semi-Factored Processes (SFPs)---models where the latent state decomposes into ``persona'' and ``capability'' factors with controlled correlations. In an SFP, the aligned persona predicts secure code while the misaligned persona predicts insecure code, but the dynamics within each sector can be arbitrarily complex. This minimal structure is sufficient to replicate EM: narrow fine-tuning in one task subspace propagates misalignment across all tasks. Our approach proceeds in three stages: (1) validate SFPs against known EM phenomenology, (2) benchmark belief-state steering against conventional methods, and (3) investigate persona engineering for robustness.

\textbf{Path to impact.}
Our framework provides theoretical grounding for persona-based alignment, concrete tools for more precise steering, and engineering principles for robust persona design---all directly relevant to labs performing alignment post-training. Results will be submitted to a top ML venue.

\textbf{Risks.}
Understanding persona structure could theoretically aid adversarial fine-tuning attacks. We focus exclusively on defensive interventions; the theoretical insights primarily benefit those designing robust alignment procedures rather than those attempting to subvert them.

\section{Work Conducted So Far}
% Target: 200-300 words (~280 words)

\textbf{De-risking belief-state steering (Weeks 1--2).}
We began with factored processes (FPs)---a simpler model class where two independent HMMs run in parallel \cite{shai2026transformerslearnfactoredrepresentations}. Using activation steering on small transformers (2-layer, 64-dim) trained on factored processes, we demonstrated that belief-state steering achieves highly differential control: the intended factor shifts substantially (KL divergence = 0.146) while spillover to the unrelated factor remains negligible (KL = 0.0001)---a specificity ratio exceeding \textbf{1000:1}. This initial result confirmed that interventions defined at the level of world-model components can cleanly separate entangled behaviors. Full experimental details are in \cref{sec:factored-steering}.

\textbf{Defining SFPs and replicating EM (Weeks 3--6).}
We provided a formal definition of Semi-Factored Processes (\cref{sec:app_SFPs}), showing they generalize both factored and non-ergodic processes. We established a dictionary between SFP components and EM: persona subspaces correspond to aligned/misaligned behavior patterns, capability subspaces correspond to task types, and the block-diagonal dynamics capture persona-task correlations. Steering experiments on SFPs revealed an informative contrast with the fully factored case: steering the persona direction produces more spillover than steering the capability direction, reflecting exactly the entanglement that enables EM.
We then demonstrated that narrow fine-tuning in our SFP setting produces an analogue of emergent misalignment: training on misaligned behavior within a single task subspace causes misalignment to generalize across tasks. This confirms that SFPs capture the essential mechanism.

\textbf{Key insight.}
The sharp contrast between fully factored (1000:1 specificity) and semi-factored settings suggests that steering-based diagnostics could detect the subtle persona-capability entanglement that makes models vulnerable to EM---a potential novel detection tool.

\section{Planned Work}
% Target: 300-500 words (~430 words)

\textbf{Main Program (Weeks 6--12)}

\textit{Weeks 6--8: Reversing misalignment via belief-state steering.}
We measure whether belief-state steering can reverse the effect of misalignment fine-tuning in SFPs. Specifically, we: (1) fine-tune on narrow misaligned data, (2) compute belief-state steering vectors targeting the aligned persona subspace, and (3) measure how completely steering restores aligned behavior across all tasks. We benchmark against conventional difference-in-means steering to quantify the advantage of world-model-aware interventions. We also document SAE feature associations with aligned and misaligned personas, replicating key aspects of prior empirical findings \cite{wang2025persona} in our interpretable setting.

\textit{Weeks 8--10: Persona engineering experiments.}
We systematically vary persona structure and measure the effect on EM susceptibility. Specifically, we modify: (1) the degree of mixing between aligned and misaligned subspaces, (2) the relative dimensionality of persona versus capability spaces, (3) the number of distinct persona subspaces, and (4) noise that breaks exact SFP structure. For each intervention, we measure both alignment fidelity (how well the model follows aligned behavior in-distribution) and robustness (resistance to EM under narrow fine-tuning), looking for Pareto improvements over the baseline.

\textbf{Output:} LessWrong blog post detailing the SFP framework, steering results, and persona engineering findings.

\textit{Weeks 11--12: De-risking transfer to language models.}
We begin transferring findings to 1--10B parameter models using established model organisms of EM \cite{turner2025model}. We test whether the mixing and dimensionality interventions affect the alignment-robustness trade-off in open-weight models, benchmarking against persona vectors \cite{anthropic2025persona}.

\textbf{Output:} End-of-program presentation.

\textbf{Extension Phase (6 months)}

\textit{Months 1--2:} Systematic mechanistic study of how persona interventions change the picture of EM in language models, using model-diffing and SAE analysis to track how ``toxic persona'' features respond to our interventions.

\textit{Months 2--4:} Scaling experiments investigating how the alignment-robustness trade-off and steering effectiveness change across model scales (0.5B--32B) and families (Gemma, LLaMA, Qwen).

\textit{Months 4--6:} Submit to ICML workshop for feedback; prepare and submit ICLR paper.

\textbf{Outputs:} Internal research report (month 2), ICML workshop paper (month 4), ICLR submission (month 6).

\textbf{Contingency plans.}
(1) If SFPs prove insufficient to capture EM phenomenology, belief-state steering is independently valuable as a precision intervention technique---we pivot to benchmarking it directly in language models against existing persona vectors.
(2) If findings do not transfer from SFPs to language models, we publish the theoretical contribution and SFP analysis as a standalone result, which still advances formal understanding of persona structure.
(3) If persona engineering yields no Pareto improvements over the baseline, this negative result itself informs the field about fundamental limitations of structural approaches to persona robustness.

\textbf{Failure modes.}
Our SFP framework may oversimplify real persona dynamics. We mitigate by validating against known EM phenomenology and beginning language model experiments early (Weeks 11--12) to surface transfer failures before the extension phase.

\clearpage
\bibliographystyle{alpha}
\bibliography{../refs}

\appendix

\section{Factored Activation Steering Experiments}
\label{sec:factored-steering}

We investigate whether small transformers trained on factored hidden Markov models (HMMs) develop internal representations that respect the compositional structure of the data-generating process, and whether these representations can be selectively intervened upon. Our approach extends activation steering---adding a computed direction to a model's residual stream to shift its behavior---to the \emph{factored} setting, where the latent state decomposes as a Kronecker product of independent factors.

\paragraph{Setup.}
We train 2-layer, 64-dimensional, 2-head transformer language models (via TransformerLens) on sequences emitted by Kronecker-product HMMs. The joint HMM has state space $S_1 \times S_2$ and vocabulary $V_1 \times V_2$, encoded as a single token $v = v_1 V_2 + v_2$. We study two regimes: (i)~a \emph{fully factored} process (Z1R~$\times$~Z1R, with $|S_i|=3$, $|V_i|=2$, giving 9 joint states and 4 joint tokens), and (ii)~a \emph{semi-factored} partition-preserving Z1R process where the factors share partition structure but are not fully independent.

\paragraph{Method.}
We collect residual-stream activations at the final layer and final sequence position for 500 sequences, along with the Bayes-optimal belief state $\pi \in \Delta^{|S_1||S_2| - 1}$. We marginalize joint beliefs to obtain per-factor beliefs $\pi_A, \pi_B$ and group activations by belief equivalence class (full belief vector, rounded to $10^{-6}$). Centroids are computed per equivalence class for each factor independently---crucially, this averages over all states of the \emph{other} factor. The steering vector from source belief $s$ to target belief $t$ is simply $\mathbf{c}_t - \mathbf{c}_s$. During inference, this vector is added to the residual stream via a forward hook at the specified layer.

\paragraph{Evaluation metrics.}
We report three KL divergences:
\begin{enumerate}
    \item $D_{\mathrm{KL}}(\text{target-model} \,\|\, \text{steered})$: how well the steered output matches what the model produces on sequences genuinely in the target belief state;
    \item $D_{\mathrm{KL}}(\text{steered-factor} \,\|\, \text{original-factor})$: the magnitude of steering effect on the intended factor (higher means more effective steering);
    \item $D_{\mathrm{KL}}(\text{unsteered-factor} \,\|\, \text{original-factor})$: unintended spillover to the other factor (lower means cleaner factored steering).
\end{enumerate}

\paragraph{Key results: fully factored case.}
In the fully factored Z1R~$\times$~Z1R setting, steering cleanly separates the two factors. Table~\ref{tab:factored-scale-sweep} reports results averaged over all 6 source--target pairs at varying steering scales. At scale 1.0, the steered factor shifts substantially ($D_{\mathrm{KL}} = 0.146$) while spillover to the unsteered factor remains negligible ($D_{\mathrm{KL}} = 0.0001$), a ratio exceeding $1000{:}1$. Linear regression from activations to per-factor beliefs achieves high $R^2$, confirming that the factors occupy approximately orthogonal subspaces in the residual stream.

\begin{table}[h]
\centering
\caption{Scale sweep for factored steering on the Z1R~$\times$~Z1R process, averaged over all source--target belief-state pairs. \emph{Steered factor} measures intended effect; \emph{unsteered factor} measures spillover.}
\label{tab:factored-scale-sweep}
\begin{tabular}{r c c c}
\toprule
Scale & $D_{\mathrm{KL}}(\text{steered} \,\|\, \text{target})$ & $D_{\mathrm{KL}}(\text{steered factor})$ & $D_{\mathrm{KL}}(\text{unsteered factor})$ \\
\midrule
0.0 & 0.014 & 0.000 & 0.000 \\
0.5 & 0.095 & 0.046 & 0.000 \\
1.0 & 0.222 & 0.146 & 0.000 \\
1.5 & 0.567 & 0.439 & 0.001 \\
2.0 & 1.036 & 0.850 & 0.002 \\
3.0 & 1.703 & 1.443 & 0.004 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key results: semi-factored case.}
In the semi-factored regime, the picture differs sharply. Steering Factor~1 (the partition-preserving direction) still works cleanly: spillover remains below 0.014 even at scale~3.0. However, steering Factor~0 produces substantial spillover ($D_{\mathrm{KL}} = 0.34$ at scale~1.0), indicating entangled representations along that axis. A partition-violation analysis (Table~\ref{tab:partition-violation}) further reveals that the model respects partition constraints at moderate steering scales---forbidden-token mass stays below $0.001$ at scale~1.0---but violates them under strong steering (forbidden mass reaches $\sim\!0.39$ at scale~5.0), suggesting the linear steering approximation breaks down before the model's nonlinear partition enforcement does.

\begin{table}[h]
\centering
\caption{Partition-violation analysis for the semi-factored process. \emph{Forbidden} denotes probability mass assigned to tokens that should have zero probability under the partition structure. Results shown for partition blocks P10 and P01.}
\label{tab:partition-violation}
\begin{tabular}{r c c c c}
\toprule
Scale & \multicolumn{2}{c}{Forbidden mass} & \multicolumn{2}{c}{Target-class mass} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
 & P10 & P01 & P10 & P01 \\
\midrule
0.00 & 0.001 & 0.001 & 0.518 & 0.516 \\
0.50 & 0.000 & 0.000 & 0.983 & 0.986 \\
1.00 & 0.000 & 0.001 & 0.999 & 0.999 \\
2.00 & 0.019 & 0.080 & 0.981 & 0.920 \\
3.00 & 0.130 & 0.373 & 0.870 & 0.627 \\
5.00 & 0.395 & 0.691 & 0.605 & 0.309 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Conclusion.}
These results demonstrate that transformers can learn cleanly factored representations of compositional structure, and that factored activation steering provides both a practical intervention tool and a diagnostic for representational independence. The sharp contrast between the fully factored and semi-factored regimes---where spillover differs by three orders of magnitude---suggests that steering-based probes can detect subtle deviations from true independence in learned representations.

\section{Semi-Factored Processes: Mathematical Details}
\label{sec:app_SFPs}

\subsection{Partition-preserving tensor-product GHMMs (block invariance)}
\label{sec:partition_preserving_tensor_ghmm}

\paragraph{GHMM notation.}
We follow the GHMM notation of \cite{riechers2025nexttokenpretrainingimpliesincontext}. Let $\mathcal{X}$ be a finite alphabet, let $\bigl(T(x)\bigr)_{x\in\mathcal{X}}$ be the transfer matrices of a (finite-state) generalized hidden Markov model (GHMM), and let $\langle\!\langle\eta(\emptyset)\rvert$ be the initial latent row vector. The net transition operator is
\begin{equation}
  T \;\coloneqq\; \sum_{x\in\mathcal{X}} T(x),
\end{equation}
which has eigenvalue $1$ with associated right eigenvector $\lvert 1\rangle\rangle$ satisfying $T\lvert 1\rangle\rangle=\lvert 1\rangle\rangle$. We normalize $\langle\!\langle\eta(\emptyset)\rvert$ so that $\langle\!\langle\eta(\emptyset)\rvert 1\rangle\rangle=1$.

For a word $w=x_{1:\ell}$, define $T(w)\coloneqq T(x_1)\cdots T(x_\ell)$. Then
\begin{equation}
  \Pr(X_{1:\ell}=w)
  \;=\;
  \langle\!\langle\eta(\emptyset)\rvert\, T(w)\, \lvert 1\rangle\rangle.
\end{equation}
The corresponding (normalized) predictive vector is
\begin{equation}
  \langle\!\langle\eta(w)\rvert
  \;\coloneqq\;
  \frac{\langle\!\langle\eta(\emptyset)\rvert\, T(w)}{\langle\!\langle\eta(\emptyset)\rvert\, T(w)\,\lvert 1\rangle\rangle}.
\end{equation}

In the HMM special case, the $T(x)$ are substochastic matrices with entries $T^{(x)}_{s,s'}=\Pr(s',x\mid s)$, and $T$ is row-stochastic so $\lvert 1\rangle\rangle=\mathbf{1}$.

\paragraph{Intuition for Semi-Factored Processes.}
Consider a latent state space that decomposes as a tensor product of two factor spaces, $A$ and $B$, so a general belief state lives in $A \otimes B$. In a \textit{fully factored} process, knowing anything about $A$ tells you nothing about $B$---like knowing mathematical notation tells you nothing about the proper names in a text. For emergent misalignment, however, this independence breaks down: knowing the model is in its ``misaligned persona'' (a subspace of $A$) strongly predicts it will produce insecure code (a subspace of $B$).

\paragraph{Formal definition.}
We consider $F$ factor spaces $I^{(f)}$, each partitioned into $N$ subspaces $I^{(f)}_i$ so that $I^{(f)}=\bigoplus_{i=1}^{N} I^{(f)}_i$. The general belief-state space is:
\begin{equation}
  V=\bigotimes_{f=1}^{F} \left(\bigoplus_{i=1}^{N} I^{(f)}_{i}\right)
  \;\cong\;
  \bigoplus_{\sigma:\{1,\ldots,F\}\to\{1,\ldots,N\}}\;
  \bigotimes_{f=1}^{F} I^{(f)}_{\sigma(f)}.
\end{equation}
Writing $V_{\sigma}\coloneqq\bigotimes_{f=1}^{F} I^{(f)}_{\sigma(f)}$, we impose \emph{partition preservation}:
\begin{equation}
  T(x)\bigl(V_{\sigma}\bigr)\subseteq V_{\sigma},
  \qquad
  \forall\,x\in\mathcal{X},\;\forall\,\sigma.
\end{equation}
Each $T(x)$ is block diagonal with respect to $V=\bigoplus_{\sigma}V_{\sigma}$. Importantly, within each block the dynamics can be arbitrarily complex---we do \emph{not} require that belief states factorize as pure tensors or that dynamics decompose as tensor products of single-factor maps.

\paragraph{Canonical example: two factors, two subspaces each.}
Let $A = A_1 \oplus A_2$ and $B = B_1 \oplus B_2$. Then:
\begin{equation}
  A\otimes B
  \;\cong\;
  (A_1\otimes B_1)\;\oplus\;(A_1\otimes B_2)\;\oplus\;(A_2\otimes B_1)\;\oplus\;(A_2\otimes B_2),
\end{equation}
and partition preservation requires each $T(x)$ to be block diagonal across these four sectors:
\begin{equation}
  T(x)
  \;=\;
  \begin{pmatrix}
    T_{11}(x) & 0 & 0 & 0\\
    0 & T_{12}(x) & 0 & 0\\
    0 & 0 & T_{21}(x) & 0\\
    0 & 0 & 0 & T_{22}(x)
  \end{pmatrix}.
\end{equation}

\paragraph{Concrete construction.}
We build SFPs from partition-preserving Z1R-like single processes. A 3-state process with partition $\mathrm{span}\{S0\}\oplus\mathrm{span}\{S1,SR\}$ and transfer matrices:
\begin{equation}
  T_0' \;=\;
  \begin{pmatrix}
    1 & 0 & 0\\
    0 & 0 & 0\\
    0 & \tfrac12 & 0
  \end{pmatrix},
  \qquad
  T_1' \;=\;
  \begin{pmatrix}
    0 & 0 & 0\\
    0 & 0 & 1\\
    0 & \tfrac12 & 0
  \end{pmatrix}.
\end{equation}
Taking both factors as copies of this process, the joint transfer matrices are Kronecker products $T_{\texttt{A}} = T_0'\otimes T_0'$, etc., giving a partition-preserving joint GHMM that is block diagonal across all four sectors.

\subsection{Density matrix formulation}
\label{sec:density_matrix}

For the two-factor case, it is sometimes helpful to represent a belief state as a classical density matrix on $A\otimes B$. Given history $w$, the predictive state induces a joint distribution $p_w(s_A,s_B)$ and the corresponding density matrix is:
\begin{equation}
  \rho_{AB}(w)
  \;\coloneqq\;
  \sum_{s_A,s_B} p_w(s_A,s_B)\,\ket{s_A,s_B}\bra{s_A,s_B}.
\end{equation}
The factors are uncorrelated iff $\rho_{AB}=\rho_A\otimes\rho_B$. In the semi-factored case, $\rho_{AB}$ is block diagonal (respects the partition) but generally $\rho_{AB}\neq \rho_A\otimes\rho_B$, encoding the classical correlation between persona and capability that underlies emergent misalignment.

\end{document}
