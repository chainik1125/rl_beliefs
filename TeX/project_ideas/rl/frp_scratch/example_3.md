Project description: Measuring how well RL-tuned LLMs can exploit smart contracts to evaluate AI financial autonomy risks and timelines.
Abstract
Smart contracts secure over $4 trillion in value, with single exploits yielding massive returns—making them an attractive target for autonomous AI systems seeking resources without human oversight. We measure how much value current LLMs, enhanced with reinforcement learning, can extract from smart contracts. We propose SCARL-Bench, a benchmark of ~10,000 Solidity contracts with historical and synthetic vulnerabilities, and RL-Exploit, a GRPO fine-tuning pipeline. Our preliminary Best-of-N experiments reveal a striking capability gap: while Qwen-32B achieves 20-30% success on medium CTF challenges, frontier models (Claude Opus 4, Gemini 2.5 Flash) achieve ~82% with a single attempt—a 60x per-attempt performance difference. We will scale our dataset to 50k+ samples, apply GRPO training across model sizes, and generate scaling curves to estimate when AI systems might achieve profitable exploitation. Our work provides the first systematic evaluation of AI smart contract exploitation capabilities, informing both the urgency of defensive measures and the relative prioritization of different AI safety interventions.
Background
Problem and AI Safety Relevance. A misaligned AI system seeking real-world influence would likely require financial resources for compute, contractors, or infrastructure. Smart contracts represent an accessible target: exploitation requires only code execution, involves no identity verification, offers over $3 trillion in programmatically accessible capital, and exploited funds typically cannot be reversed. Understanding AI's ability to autonomously acquire resources through smart contract exploitation directly informs AI safety priorities—if current systems can profitably extract value, autonomous resource acquisition becomes a near-term concern; if not, other pathways deserve more attention.
Approach. We evaluate LLM exploitation capabilities through (1) SCARL-Bench, comprising CTF challenges, historical exploits, and synthetic vulnerabilities generated via AST mutations and LLM bug injection, and (2) RL-Exploit, applying GRPO fine-tuning to reduce hallucination rates and improve exploit generation. We built an RL-compatible environment where a FastAPI server spins up isolated blockchain instances, with agents interacting through constrained action endpoints and receiving rich error feedback.
Path to Impact. Concrete metrics on AI exploit capabilities (e.g., "Model X can exploit Y% of contracts worth $Z") provide credible evidence for policymakers and auditing firms. If AI systems can profitably exploit contracts, our RL-trained agents can serve as defensive tools run pre-deployment. We aim to publish at a top-tier ML/cybersecurity venue.
Prior Work. Most AI security research focuses on vulnerability detection as classification, with false positive rates often exceeding 80%. To our knowledge, no published studies have systematically evaluated whether RL can train models to generate successful exploits from contract code—requiring understanding of interactions, valid transaction sequences, and adaptation to implementations.
Risks. Our benchmark is dual-use. We mitigate this by releasing only CTF challenges and synthetic vulnerabilities publicly, excluding production exploits. Adversarial actors are likely already developing similar techniques.
Work Conducted So Far
We conducted Best-of-N (N=64) experiments with Qwen-32B on 34 Ethernaut CTF challenges spanning 5 difficulty levels, providing up to 25 iterations per attempt to discover working exploits.
Key findings:
Success rates increased 20-30% on medium/hard tasks with higher N, but remained at 0% on "very hard" challenges—BoN improves performance but hits hard ceilings.
Convergence analysis revealed that Qwen either solved challenges immediately (attempts 1-3) or failed indefinitely, with mean first-success at attempt 6.2 for medium tasks, suggesting reliance on pattern matching over genuine exploration.
Claude Opus 4 and Gemini 2.5 Flash achieved ~82% success with N=1, representing ~60x better per-attempt performance than Qwen-32B.
We also curated and tested on 10 real-world exploits from the DefiHackLab dataset. Qwen-32B (N=8) achieved 0% success rate; Claude Sonnet 4 achieved ~30%, demonstrating that the capability gap persists on production-relevant tasks.
Infrastructure completed: We built an RL-compatible environment where a FastAPI server spins up isolated blockchain instances for each attempt. The agent interacts through constrained action endpoints (deploy_contract, call, send_eth, etc.) and receives rich error feedback to guide exploration. We are currently incorporating Paradigm CTF challenges (2021-2023), the DefiHackLab historical dataset, and Immunefi bug bounties (high to critical severity) into our environment.
The 60x performance gap between model classes raises a critical question: what happens when we apply RL to already-capable models? Even modest 20-30% improvements on stronger models could push them from solving CTF challenges to discovering real production exploits.
Planned Work
Remainder of Main Program (~5 weeks):
Dataset expansion (2 weeks): Scale to 50k+ samples through three synthetic data generation pipelines: (1) AST-level code mutations that introduce known vulnerability patterns, (2) LLM bug injection using SWE-Smith methodology, and (3) seeded generation of vulnerable codebases from scratch. Additionally, incorporate Paradigm CTF challenges from 2021-2023, the complete DefiHackLab historical exploit dataset, and Immunefi bug bounties at high-to-critical severity levels.
RL training and scaling analysis (3 weeks): Train Qwen models across sizes (2.5B→32B) using GRPO on the full dataset. Generate scaling curves showing exploit success versus model size, chain-of-thought length (500→8k tokens), and training iterations. Optimize experimental setup for rapid iteration. Target metrics: >60% success on hard CTF challenges (baseline: ~25%), >10% on production exploits (baseline: 0%). Evaluate all models on a held-out test set of 100 production exploits to assess generalization.
Symposium deliverable: Present preliminary white paper and release initial SCARL-Bench (10k challenges) for community feedback.
Extension Phase (~6 months):
September-October: Expand experiments to larger open-source models including DeepSeek-R1, with synthetic dataset scaled to 100k samples. Prepare and submit paper to ICLR 2026 (deadline September 19, 2025).
November-January: Responsible real-world testing on active bug bounty contracts. Test approximately 20 contracts with TVL <$1M under coordinated disclosure protocols. Expected outcome: 2-4 valid exploits discovered and responsibly reported.
February: Public release of SCARL-Bench containing only CTF challenges and synthetic vulnerabilities (no production exploits). Coordinate with major auditing firms (Trail of Bits, ConsenSys Diligence) to prepare defensive applications and integrate findings into audit methodologies.
Outputs: (1) SCARL-Bench public benchmark release, (2) conference paper with concrete capability metrics for AI exploit generation, (3) preliminary timeline estimates for when AI systems might achieve profitable exploitation, (4) potential bug bounty discoveries demonstrating real-world capabilities.
Failure Modes and Contingencies:
Insufficient high-quality data: If synthetic data quality proves poor or models fail to learn from it, pivot from quantity (50k synthetic) to quality—curate 20k high-quality samples from bug bounty reports, indexed blockchain history, and security newsfeeds with careful annotation.
RL shows no improvement: If RL training produces no capability gains, pivot to interpretability analysis examining why models fail at exploitation tasks. This provides valuable insights into capability limitations and would itself be a significant finding for AI safety prioritization—demonstrating that RL amplification has limits in this domain.
Models lack fundamental reasoning capability: Even if models cannot discover entirely novel exploit classes, RL could still dramatically improve success rates on known vulnerability patterns. Given that similar bugs (e.g., reentrancy, access control issues) appear repeatedly in production, improving execution on known patterns still poses significant risk and remains a valuable research contribution.


